{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPb93CiGTexdpTCBumVetZQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","\n","\n","# Language Modeling\n","**bold text**\n","Language modeling is a fundamental task in Natural Language Processing (NLP) that involves predicting the probability of a sequence of words in a language. It is used to understand and generate human language by training models on large amounts of text data."],"metadata":{"id":"pYsBHM_Bv_bI"}},{"cell_type":"markdown","source":["# **Language Modeling Using N-gram Model**\n","\n","---\n","\n","\n","\n","An N-gram model predicts the next word based on the previous n-1 words. Implementation of a bigram model (n=2):"],"metadata":{"id":"YsZOLHUxwJAm"}},{"cell_type":"markdown","source":["**Approach:**\n","- Predict the next word based on the previous n-1 words.\n","- Use statistical probabilities derived from the frequency of word sequences in the training data.\n","\n","**Pros:**\n","- Simple and easy to implement.\n","- Efficient for small datasets and limited computational resources.\n","- Can capture local dependencies and short-term context.\n","\n","**Cons:**\n","- Struggle with long-range dependencies and capturing the broader context.\n","- Suffer from data sparsity, especially for higher values of n.\n","- Limited ability to handle rare or unseen word sequences."],"metadata":{"id":"cTlXlG5FGMRz"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sB1t8SkhvrZh","executionInfo":{"status":"ok","timestamp":1744121789086,"user_tz":240,"elapsed":40,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}},"outputId":"4b9ff9a8-2ba0-43e3-fc2f-1043643af1fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["dog\n"]}],"source":["from collections import defaultdict, Counter\n","import random\n","\n","class BigramModel:\n","    def __init__(self):\n","        self.bigrams = defaultdict(Counter)\n","\n","    def train(self, sentences):\n","        for sentence in sentences:\n","            tokens = sentence.split()\n","            for i in range(len(tokens) - 1):\n","                self.bigrams[tokens[i]][tokens[i + 1]] += 1\n","\n","    def predict_next_word(self, word):\n","        if word in self.bigrams:\n","            next_words = self.bigrams[word]\n","            total = sum(next_words.values())\n","            r = random.uniform(0, total)\n","            upto = 0\n","            for next_word, count in next_words.items():\n","                if upto + count >= r:\n","                    return next_word\n","                upto += count\n","        return None\n","\n","\n","sentences = [\n","    \"the cat sat on the mat\",\n","    \"the dog barked at the cat\",\n","    \"the cat chased the mouse\"\n","]\n","\n","model = BigramModel()\n","model.train(sentences)\n","print(model.predict_next_word(\"the\"))"]},{"cell_type":"markdown","source":["# **Language Modeling Using Neural Language Model**\n","\n","A neural language model uses a neural network to predict the next word in a sequence. Implementation using a recurrent neural network (RNN) with PyTorch:"],"metadata":{"id":"O2xkVowPwwMc"}},{"cell_type":"markdown","source":["**Approach:**\n","- Use neural networks, such as feedforward networks or recurrent neural networks (RNNs), to model the probability distribution of sequences of words.\n","- Learn continuous representations of words (embeddings) and capture complex patterns in the data.\n","\n","**Pros:**\n","- Can capture longer-range dependencies compared to N-gram models.\n","- Learn rich, dense representations of words that capture semantic similarities.\n","- Adaptable to various NLP tasks with transfer learning.\n","\n","**Cons:**\n","- Require large amounts of data and computational resources for training.\n","- Can be difficult to interpret and understand the internal representations.\n","- May struggle with rare words or out-of-vocabulary items without proper handling."],"metadata":{"id":"BTFpbp4aGVs4"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","\n","# Sample data\n","sentences = [\n","    \"the cat sat on the mat\",\n","    \"the dog barked at the cat\",\n","    \"the cat chased the mouse\"\n","]\n","\n","# Tokenize the text\n","tokenizer = lambda x: x.split()\n","vocab = set(word for sentence in sentences for word in tokenizer(sentence))\n","word2idx = {word: idx for idx, word in enumerate(vocab)}\n","idx2word = {idx: word for word, idx in word2idx.items()}\n","\n","# Prepare input-output pairs\n","input_seq = []\n","output_seq = []\n","for sentence in sentences:\n","    tokens = tokenizer(sentence)\n","    for i in range(len(tokens) - 1):\n","        input_seq.append(word2idx[tokens[i]])\n","        output_seq.append(word2idx[tokens[i + 1]])\n","\n","input_seq = np.array(input_seq)\n","output_seq = np.array(output_seq)\n","\n","# Convert to PyTorch tensors\n","input_tensor = torch.from_numpy(input_seq).long()\n","output_tensor = torch.from_numpy(output_seq).long()\n","\n","# Define the RNN model\n","class RNNLanguageModel(nn.Module):  # Fix 1: Inherit from nn.Module\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim):  # Fix 2: Correct __init__ method syntax\n","        super(RNNLanguageModel, self).__init__()  # Fix 3: Call parent constructor\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, vocab_size)\n","        self.hidden_dim = hidden_dim\n","\n","    def forward(self, x, hidden):\n","        x = self.embedding(x)\n","        x = x.view(x.size(0), 1, -1)  # Fix 4: Reshape for batch_first=True\n","        out, hidden = self.rnn(x, hidden)\n","        out = self.fc(out.squeeze(1))\n","        return out, hidden\n","\n","    def init_hidden(self, batch_size=1):\n","        return torch.zeros(1, batch_size, self.hidden_dim)  # Fix 5: Correct hidden state dimensions\n","\n","# Hyperparameters\n","vocab_size = len(vocab)\n","embedding_dim = 64\n","hidden_dim = 128\n","learning_rate = 0.01\n","num_epochs = 100\n","\n","# Initialize the model, loss function, and optimizer\n","model = RNNLanguageModel(vocab_size, embedding_dim, hidden_dim)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Fix 6: No need for custom parameters method\n","\n","# Train the model\n","for epoch in range(num_epochs):\n","    total_loss = 0\n","    hidden = model.init_hidden()  # Fix 7: Initialize hidden state once per epoch\n","\n","    for i in range(len(input_tensor)):\n","        optimizer.zero_grad()\n","\n","        input_word = input_tensor[i].unsqueeze(0)  # Add batch dimension\n","        target = output_tensor[i].unsqueeze(0)  # Fix 8: Rename for clarity\n","\n","        output, hidden = model(input_word, hidden)  # Fix 9: Use direct call syntax\n","        hidden = hidden.detach()  # Fix 10: Detach hidden state to prevent backprop through entire sequence\n","\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    if (epoch + 1) % 10 == 0:\n","        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss/len(input_tensor):.4f}')\n","\n","# Generate text\n","def generate_text(model, start_word, length=10):\n","    model.eval()\n","    with torch.no_grad():  # Fix 11: Add no_grad context for inference\n","        hidden = model.init_hidden()\n","        input_word = torch.tensor([word2idx[start_word]]).long()  # Fix 12: Specify tensor dtype\n","        generated_words = [start_word]\n","\n","        for _ in range(length):\n","            output, hidden = model(input_word, hidden)\n","            output_word_idx = torch.argmax(output, dim=1).item()  # Fix 13: Specify argmax dimension\n","            output_word = idx2word[output_word_idx]\n","            generated_words.append(output_word)\n","            input_word = torch.tensor([output_word_idx]).long()\n","\n","    return ' '.join(generated_words)\n","\n","print(generate_text(model, \"the\"))"],"metadata":{"id":"QaEfd6phwl61","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744126821028,"user_tz":240,"elapsed":2473,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}},"outputId":"61fd1416-5264-4d80-ac25-23394050a389"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [10/100], Loss: 0.4509\n","Epoch [20/100], Loss: 0.0084\n","Epoch [30/100], Loss: 0.0041\n","Epoch [40/100], Loss: 0.0025\n","Epoch [50/100], Loss: 0.0017\n","Epoch [60/100], Loss: 0.0013\n","Epoch [70/100], Loss: 0.0010\n","Epoch [80/100], Loss: 0.0008\n","Epoch [90/100], Loss: 0.0006\n","Epoch [100/100], Loss: 0.0005\n","the cat sat on the mat dog barked at the cat\n"]}]},{"cell_type":"markdown","source":["# Language Modeling Using Transformer Model"],"metadata":{"id":"BA32k_61E7DY"}},{"cell_type":"markdown","source":["**Approach:**\n","- Use self-attention mechanisms to weigh the importance of input words relative to each other, regardless of their distance.\n","- Models like BERT, T5, and others have achieved state-of-the-art performance on various NLP tasks.\n","\n","**Pros:**\n","- Excellent at capturing long-range dependencies and contextual information.\n","- Highly parallelizable, leading to faster training times compared to RNNs.\n","- Can be fine-tuned for specific tasks with transfer learning.\n","\n","**Cons:**\n","- Require massive amounts of data and computational resources for pre-training.\n","- Can be memory-intensive due to the need to store attention weights.\n","- May require careful tuning and adaptation for domain-specific tasks."],"metadata":{"id":"H0JnDY1cGbpa"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForMaskedLM\n","import torch\n","\n","# Load pre-trained BERT model and tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n","\n","# Example sentence\n","sentence = \"The cat sat on the [MASK].\"\n","inputs = tokenizer(sentence, return_tensors='pt')\n","\n","# Predict the masked token\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","    predictions = outputs.logits\n","\n","# Get the predicted token\n","predicted_token_id = torch.argmax(predictions[0, inputs['input_ids'][0] == tokenizer.mask_token_id])\n","predicted_token = tokenizer.decode([predicted_token_id])\n","print(predicted_token)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QIFWiLJEAKQN","executionInfo":{"status":"ok","timestamp":1744127234119,"user_tz":240,"elapsed":954,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}},"outputId":"8f844a31-9428-4d6e-8473-9b571557fb41"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["floor\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"qO5dgc5yFCBo"},"execution_count":null,"outputs":[]}]}